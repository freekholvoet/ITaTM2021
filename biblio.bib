@article{Antonio2012,
abstract = {Everyday we face all kinds of risks, and insurance is in the business of providing us a means to transfer or share these risks, usually to eliminate or reduce the resulting financial burden, in exchange for a predetermined price or tariff. Actuaries are considered professional experts in the economic assessment of uncertain events, and equipped with many statistical tools for analytics, they help formulate a fair and reasonable tariff associated with these risks. An important part of the process of establishing fair insurance tariffs is risk classification, which involves the grouping of risks into various classes that share a homogeneous set of characteristics allowing the actuary to reasonably price discriminate. This article is a survey paper on the statistical tools for risk classification used in insurance. Because of recent availability of more complex data in the industry together with the technology to analyze these data, we additionally discuss modern techniques that have recently emerged in the statistics discipline and can be used for risk classification. While several of the illustrations discussed in the paper focus on general, or non-life, insurance, several of the principles we examine can be similarly applied to life insurance. Furthermore, we also distinguish between a priori and a posteriori ratemaking. The former is a process which forms the basis for ratemaking when a policyholder is new and insufficient information may be available. The latter process uses additional historical information about policyholder claims when this becomes available. In effect, the resulting a posteriori premium allows one to correct and adjust the previous a priori premium making the price discrimination even more fair and reasonable. {\textcopyright} 2011 The Author(s).},
author = {Antonio, Katrien and Valdez, Emiliano A.},
doi = {10.1007/s10182-011-0152-7},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Antonio, Valdez - 2012 - Statistical concepts of a priori and a posteriori risk classification in insurance.pdf:pdf},
issn = {18638171},
journal = {AStA Advances in Statistical Analysis},
keywords = {Actuarial science,Bonus-Malus systems,Regression and credibility models},
number = {2},
pages = {187--224},
title = {{Statistical concepts of a priori and a posteriori risk classification in insurance}},
volume = {96},
year = {2012}
}
@book{Bishop1993,
abstract = {This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.},
author = {Bishop, Christopher M},
booktitle = {Advances in Computers},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bishop - 1995 - Neural Networks for Pattern Recognition.pdf:pdf},
isbn = {978-0-19-853864-6},
title = {{Neural Networks for Pattern Recognition}},
year = {1995}
}
@article{Blier-Wong2021,
abstract = {Insurance companies gather a growing variety of data for use in the insurance process, but most traditional ratemaking models are not designed to support them. In particular, many emerging data sources (text, images, sensors) may complement traditional data to provide better insights to predict the future losses in an insurance contract. This paper presents some of these emerging data sources and presents a unified framework for actuaries to incorporate these in existing ratemaking models. Our approach stems from representation learning, whose goal is to create representations of raw data. A useful representation will transform the original data into a dense vector space where the ultimate predictive task is simpler to model. Our paper presents methods to transform non-vectorial data into vectorial representations and provides examples for actuarial science.},
archivePrefix = {arXiv},
arxivId = {2102.05784},
author = {Blier-Wong, Christopher and Baillargeon, Jean-Thomas and Cossette, H{\'{e}}l{\`{e}}ne and Lamontagne, Luc and Marceau, Etienne},
eprint = {2102.05784},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blier-Wong et al. - 2021 - Rethinking Representations in P&C Actuarial Science with Deep Neural Networks.pdf:pdf},
keywords = {embeddings,emerging data,feature learning,machine learning,neural networks,pricing,property and casualty insurance,unstructured data},
month = {feb},
pages = {1--27},
title = {{Rethinking Representations in P&C Actuarial Science with Deep Neural Networks}},
url = {http://arxiv.org/abs/2102.05784},
year = {2021}
}
@article{breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, * * * , 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 2001 - Random Forests.pdf:pdf},
journal = {Machine Learning},
keywords = {classification,ensemble,regression},
pages = {5--32},
title = {{Random Forests}},
volume = {45},
year = {2001}
}
@article{breiman1996,
abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
author = {Breiman, Leo},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 1996 - Bagging Predictors.pdf:pdf},
journal = {Mach Learn},
keywords = {Aggregation,Averaging,Bootstrap,Combining},
pages = {123--140},
publisher = {Kluwer Academic Publishers},
title = {{Bagging Predictors}},
volume = {24},
year = {1996}
}
@book{breiman1984,
author = {Breiman, Leo and Friedman, Jerome and Olshen, Richard A. and Stone, Charles J.},
doi = {10.1201/9781315139470},
isbn = {9781315139470},
month = {oct},
publisher = {Routledge},
title = {{Classification And Regression Trees}},
url = {https://www.taylorfrancis.com/books/9781351460491},
year = {1984}
}
@article{Christmann2004,
abstract = {This paper describes common features in data sets from motor vehicle insurance companies and proposes a general approach which exploits knowledge of such features in order to model high–dimensional data sets with a complex dependency structure. The results of the approach can be a basis to develop insurance tariffs. The approach is applied to a collection of data sets from several motor vehicle insurance companies. As an example, we use a nonparametric approach based on a combination of two methods from modern statistical machine learning, i.e. kernel logistic regression and $\epsilon$-support vector regression.},
author = {Christmann, Andreas},
doi = {10.1007/s101820400178},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Christmann - 2004 - An approach to model complex high-dimensional insurance data.pdf:pdf},
issn = {0002-6018},
journal = {Allgemeines Statistisches Archiv},
keywords = {c10,c13,c14,chine learning,classification,data mining,insurance tariffs,jel,kernel logistic regression,ma-,regression,robustness,simplicity,support vector,support vector machine},
number = {4},
pages = {375--396},
title = {{An approach to model complex high-dimensional insurance data}},
volume = {88},
year = {2004}
}
@article{Czado2012,
abstract = {A crucial assumption of the classical compound Poisson model of Lundberg for assessing the total loss incurred in an insurance portfolio is the independence between the occurrence of a claim and its claims size. In this paper we present a mixed copula approach suggested by Song et al. to allow for dependency between the number of claims and its corresponding average claim size using a Gaussian copula. Marginally we permit for regression effects both on the number of incurred claims as well as its average claim size using generalized linear models. Parameters are estimated using adaptive versions of maximization by parts (MBP). The performance of the estimation procedure is validated in an extensive simulation study. Finally the method is applied to a portfolio of car insurance policies, indicating its superiority over the classical compound Poisson model. {\textcopyright} 2012 Copyright Taylor and Francis Group, LLC.},
author = {Czado, Claudia and Kastenmeier, Rainer and Brechmann, Eike Christian and Min, Aleksey},
doi = {10.1080/03461238.2010.546147},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Czado et al. - 2012 - A mixed copula model for insurance claims and claim sizes.pdf:pdf},
issn = {03461238},
journal = {Scandinavian Actuarial Journal},
keywords = {Average claim size,Copula,GLM,Maximization by parts,Number of claims,Total claim size},
number = {4},
pages = {278--305},
title = {{A mixed copula model for insurance claims and claim sizes}},
year = {2012}
}
@article{DalPozzolo2011,
abstract = {Overview about DM algorithms (supervised, learning, Classification, Regression, Unsupervised learning);},
author = {{Dal Pozzolo}, Andrea},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dal Pozzolo - 2011 - Comparison of Data Mining Techniques for Insurance Claim Prediction.pdf:pdf},
journal = {Journal Article},
pages = {11},
title = {{Comparison of Data Mining Techniques for Insurance Claim Prediction}},
url = {http://www.ulb.ac.be/di/map/adalpozz/pdf/Claim_prediction.pdf},
year = {2011}
}
@article{David2015,
abstract = {It seems more and more that we live in a society afraid of everything, where everything can be considered as risk taking. This feeling of uncertainty and fear leads many individuals to manifest a great interest for safety. In the context of a risky society, the requirement for insurances is becoming more and more pronounced, the main concerns of the insureds being the guarantee of financial safety and security against a possible loss on a particular event. The entire process of insurance consists in offering an equitable method of transferring the risk in exchange for a predetermined price or tariff. This article is a review paper that describes the fundamental concepts of insurance pricing and reviews the main statistical tools used in insurance to reasonably discriminate the price.},
author = {David, Mihaela},
doi = {10.1016/s2212-5671(15)00060-x},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/David - 2015 - A Review of Theoretical Concepts and Empirical Literature of Non-life Insurance Pricing.pdf:pdf},
issn = {22125671},
journal = {Procedia Economics and Finance},
keywords = {a posteriori pricing,a priori pricing,actuarial science,bonus-malus system,credibility theory,non-life insurance pricing,pure premium,risk classification,tariff class},
number = {15},
pages = {157--162},
publisher = {Elsevier B.V.},
title = {{A Review of Theoretical Concepts and Empirical Literature of Non-life Insurance Pricing}},
url = {http://dx.doi.org/10.1016/S2212-5671(15)00060-X},
volume = {20},
year = {2015}
}
@book{Heller2014,
address = {Cambridge},
author = {{De Jong}, Piet and Heller, Gillian Z.},
booktitle = {International Statistical Review},
doi = {10.1017/CBO9780511755408},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/De Jong, Heller - 2008 - Generalized Linear Models for Insurance Data.pdf:pdf},
isbn = {9780511755408},
issn = {03067734},
keywords = {Heller and Jong},
month = {aug},
number = {2},
pages = {315--315},
publisher = {Cambridge University Press},
title = {{Generalized Linear Models for Insurance Data}},
url = {http://doi.wiley.com/10.1111/j.1751-5823.2008.00054_17.x http://ebooks.cambridge.org/ref/id/CBO9780511755408},
volume = {76},
year = {2008}
}
@article{denuit2004,
author = {Denuit, Michel and Lang, Stefan},
doi = {10.1016/j.insmatheco.2004.08.001},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
month = {dec},
number = {3},
pages = {627--647},
title = {{Non-life rate-making with Bayesian GAMs}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0167668704000940},
volume = {35},
year = {2004}
}
@book{denuit2007,
address = {Chichester, UK},
author = {Denuit, Michel and Marchal, Xavier and Pitrebois, Sandra and Walhin, Jean-Franois},
doi = {10.1002/9780470517420},
isbn = {9780470517420},
month = {aug},
publisher = {John Wiley \& Sons, Ltd},
title = {{Actuarial Modelling of Claim Counts}},
url = {http://doi.wiley.com/10.1002/9780470517420},
year = {2007}
}
@article{Dozat2016,
abstract = {This work aims to improve upon the recently proposed and rapidly popular-ized optimization algorithm Adam (Kingma & Ba, 2014). Adam has two main components—a momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be in-ferior to a similar algorithm known as Nesterov's accelerated gradient (NAG). We show how to modify Adam's momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned mod-els.},
author = {Dozat, Timothy},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dozat - 2016 - Incorporating Nesterov Momentum into Adam.pdf:pdf},
journal = {ICLR Workshop},
number = {1},
pages = {2013--2016},
title = {{Incorporating Nesterov Momentum into Adam}},
year = {2016}
}
@book{frees2014,
address = {Cambridge},
author = {Frees, Edward W and Derrig, Richard A and Meyers, Glenn},
doi = {10.1017/CBO9781139342674},
isbn = {9781139342674},
publisher = {Cambridge University Press},
title = {{Predictive Modeling Applications In Actuarial Science}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139342674},
year = {2014}
}
@article{Ganesan2010,
abstract = {Artificial Neural Network is a branch of Artificial intelligence, has been accepted as a new technology in computer science. Neural Networks are currently a 'hot' research area in medicine, particularly in the fields of radiology, urology, cardiology, oncology and etc. It has a huge application in many areas such as education, business; medical, engineering and manufacturing .Neural Network plays an important role in a decision support system. In this paper, an attempt has been made to make use of neural networks in the medical field (carcinogenesis (pre-clinical study)). In carcinogenesis, artificial neural networks have been successfully applied to the problems in both pre-clinical and post-clinical diagnosis. The main aim of research in medical diagnostics is to develop more cost-effective and easy–to-use systems, procedures and methods for supporting clinicians. It has been used to analyze demographic data from lung cancer patients with a view to developing diagnostic algorithms that might improve triage practices in the emergency department. For the lung cancer diagnosis problem, the concise rules extracted from the network achieve an high accuracy rate of on the training data set and on the test data set.},
author = {Ganesan, Dr. N. and Venkatesh, Dr.K. and Rama, Dr. M. A. and Palani, A. Malathi},
doi = {10.5120/476-783},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganesan et al. - 2010 - Application of Neural Networks in Diagnosing Cancer Disease using Demographic Data.pdf:pdf},
issn = {09758887},
journal = {International Journal of Computer Applications},
month = {feb},
number = {26},
pages = {81--97},
title = {{Application of Neural Networks in Diagnosing Cancer Disease using Demographic Data}},
url = {http://www.ijcaonline.org/journal/number26/pxc387783.pdf},
volume = {1},
year = {2010}
}
@misc{gdpr2016,
author = {GDPR},
title = {{Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC}},
year = {2016}
}
@mastersthesis{Gielis2020,
address = {Leuven},
author = {Gielis, Simon},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gielis - 2020 - Neural networks for non-life insurance pricing as benchmarks(2).pdf:pdf},
publisher = {KU Leuven. Faculteit Wetenschappen},
title = {{Neural networks for non-life insurance pricing as benchmarks}},
url = {https://limo.libis.be/primo-explore/fulldisplay?docid=32LIBIS_ALMA_DS71235421870001471&context=L&vid=KULeuven&search_scope=ALL_CONTENT&tab=all_content_tab&lang=en_US%0A},
year = {2020}
}
@techreport{goldstein2014,
abstract = {This article presents Individual Conditional Expectation (ICE) plots, a tool for visualizing the model estimated by any supervised learning algorithm. Classical partial dependence plots (PDPs) help visualize the average partial relationship between the predicted response and one or more features. In the presence of substantial interaction effects, the partial response relationship can be heterogeneous. Thus, an average curve, such as the PDP, can obfuscate the complexity of the modeled relationship. Accordingly, ICE plots refine the partial dependence plot by graphing the functional relationship between the predicted response and the feature for individual observations. Specifically, ICE plots highlight the variation in the fitted values across the range of a covariate, suggesting where and to what extent heterogeneities might exist. In addition to providing a plotting suite for exploratory analysis, we include a visual test for additive structure in the data generating model. Through simulated examples and real data sets, we demonstrate how ICE plots can shed light on estimated models in ways PDPs cannot. Procedures outlined are available in the R package ICEbox.},
archivePrefix = {arXiv},
arxivId = {1309.6392v2},
author = {Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
eprint = {1309.6392v2},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldstein et al. - 2014 - Peeking Inside the Black Box Visualizing Statistical Learning with Plots of Individual Conditional Expectation.pdf:pdf},
title = {{Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation}},
year = {2014}
}
@book{Goodfellow-et-al-2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org},
year = {2016}
}
@article{Gschl2005,
author = {Gschl{\"{o}}{\ss}l, Susanne and Czado, Claudia},
doi = {10.1080/03461230701414764},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gschl{\"{o}}{\ss}l, Czado - 2007 - Spatial modelling of claim frequency and claim size in non-life insurance.pdf:pdf},
issn = {0346-1238},
journal = {Scandinavian Actuarial Journal},
month = {sep},
number = {3},
pages = {202--225},
title = {{Spatial modelling of claim frequency and claim size in non-life insurance}},
url = {http://www.tandfonline.com/doi/abs/10.1080/03461230701414764},
volume = {2007},
year = {2007}
}
@article{Guelman2012,
abstract = {Gradient Boosting (GB) is an iterative algorithm that combines simple parameterized functions with "poor" performance (high prediction error) to produce a highly accurate prediction rule. In contrast to other statistical learning methods usually providing comparable accuracy (e.g.; neural networks and support vector machines), GB gives interpretable results, while requiring little data preprocessing and tuning of the parameters. The method is highly robust to less than clean data and can be applied to classification or regression problems from a variety of response distributions (Gaussian, Bernoulli, Poisson, and Laplace). Complex interactions are modeled simply, missing values in the predictors are managed almost without loss of information, and feature selection is performed as an integral part of the procedure. These properties make GB a good candidate for insurance loss cost modeling. However, to the best of our knowledge, the application of this method to insurance pricing has not been fully documented to date. This paper presents the theory of GB and its application to the problem of predicting auto "at-fault" accident loss cost using data from a major Canadian insurer. The predictive accuracy of the model is compared against the conventional Generalized Linear Model (GLM) approach. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
author = {Guelman, Leo},
doi = {10.1016/j.eswa.2011.09.058},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guelman - 2012 - Gradient boosting trees for auto insurance loss cost modeling and prediction.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Gradient boosting trees,Insurance pricing,Statistical learning},
month = {feb},
number = {3},
pages = {3659--3667},
publisher = {Pergamon},
title = {{Gradient boosting trees for auto insurance loss cost modeling and prediction}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417411013674},
volume = {39},
year = {2012}
}
@article{Society2016,
abstract = {The authors review the applications of generalized linear models to actuarial problems. This rich class of statistical model has been successfully applied in recent years to a wide range of problems, involving mortality, multiple-state models, lapses, premium rating and reserving. Selective examples of these applications are presented.},
author = {Haberman, Steven and Renshaw, Arthur E.},
doi = {10.2307/2988543},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haberman, Renshaw - 1996 - Generalized Linear Models and Actuarial Science.pdf:pdf},
issn = {00390526},
journal = {The Statistician},
keywords = {generalized linear models,life-insurance and non-life-insurance models},
number = {4},
pages = {407},
title = {{Generalized Linear Models and Actuarial Science}},
url = {https://www.jstor.org/stable/10.2307/2988543?origin=crossref},
volume = {45},
year = {1996}
}
@book{Freidmanetal2001,
abstract = {Behavioral economics tells us that emotions can profoundly affect individual behavior and decision-making. Does this also apply to societies at large, i.e. can societies experience mood states that affect their collective decision making? By extension is the public mood correlated or even predictive of economic indicators? Here we investigate whether measurements of collective mood states derived from large-scale Twitter feeds are correlated to the value of the Dow Jones Industrial Average (DJIA) over time. We analyze the text content of daily Twitter feeds by two mood tracking tools, namely OpinionFinder that measures positive vs. negative mood and Google-Profile of Mood States (GPOMS) that measures mood in terms of 6 dimensions (Calm, Alert, Sure, Vital, Kind, and Happy). We cross-validate the resulting mood time series by comparing their ability to detect the public's response to the presidential election and Thanksgiving day in 2008. A Granger causality analysis and a Self-Organizing Fuzzy Neural Network are then used to investigate the hypothesis that public mood states, as measured by the OpinionFinder and GPOMS mood time series, are predictive of changes in DJIA closing values. Our results indicate that the accuracy of DJIA predictions can be significantly improved by the inclusion of specific public mood dimensions but not others. We find an accuracy of 86.7% in predicting the daily up and down changes in the closing values of the DJIA and a reduction of the Mean Average Percentage Error (MAPE) by more than 6%. {\textcopyright} 2011 Elsevier B.V.},
address = {New York, NY},
author = {Hastie, Trevor and Friedman, Jerome and Tibshirani, Robert},
booktitle = {The Business of Giving},
doi = {10.1007/978-0-387-21606-5},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hastie, Friedman, Tibshirani - 2001 - The Elements of Statistical Learning.pdf:pdf},
isbn = {978-1-4899-0519-2},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning}},
url = {http://link.springer.com/10.1007/978-0-387-21606-5},
year = {2001}
}
@article{Henckaerts2018,
abstract = {We present a fully data driven strategy to incorporate continuous risk factors and geographical information in an insurance tariff. A framework is developed that aligns flexibility with the practical requirements of an insurance company, the policyholder and the regulator. Our strategy is illustrated with an example from property and casualty (P&C) insurance, namely a motor insurance case study. We start by fitting generalized additive models (GAMs) to the number of reported claims and their corresponding severity. These models allow for flexible statistical modeling in the presence of different types of risk factors: categorical, continuous, and spatial risk factors. The goal is to bin the continuous and spatial risk factors such that categorical risk factors result which captures the effect of the covariate on the response in an accurate way, while being easy to use in a generalized linear model (GLM). This is in line with the requirement of an insurance company to construct a practical and interpretable tariff that can be explained easily to stakeholders. We propose to bin the spatial risk factor using Fisher's natural breaks algorithm and the continuous risk factors using evolutionary trees. GLMs are fitted to the claims data with the resulting categorical risk factors. We find that the resulting GLMs approximate the original GAMs closely, and lead to a very similar premium structure.},
author = {Henckaerts, Roel and Antonio, Katrien and Clijsters, Maxime and Verbelen, Roel},
doi = {10.1080/03461238.2018.1429300},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Henckaerts et al. - 2018 - A data driven binning strategy for the construction of insurance tariff classes(2).pdf:pdf},
issn = {16512030},
journal = {Scandinavian Actuarial Journal},
keywords = {Continuous and spatial risk factors,construction of tariff classes,data driven binning,regression trees},
number = {8},
pages = {681--705},
title = {{A data driven binning strategy for the construction of insurance tariff classes}},
volume = {2018},
year = {2018}
}
@article{Henckaerts2020,
abstract = {Pricing actuaries typically operate within the framework of generalized linear models (GLMs). With the upswing of data analytics, our study puts focus on machine learning methods to develop full tariff plans built from both the frequency and severity of claims. We adapt the loss functions used in the algorithms such that the specific characteristics of insurance data are carefully incorporated: highly unbalanced count data with excess zeros and varying exposure on the frequency side combined with scarce but potentially long-tailed data on the severity side. A key requirement is the need for transparent and interpretable pricing models that are easily explainable to all stakeholders. We therefore focus on machine learning with decision trees: Starting from simple regression trees, we work toward more advanced ensembles such as random forests and boosted trees. We show how to choose the optimal tuning parameters for these models in an elaborate cross-validation scheme. In addition, we present visualization tools to obtain insights from the resulting models, and the economic value of these new modeling approaches is evaluated. Boosted trees outperform the classical GLMs, allowing the insurer to form profitable portfolios and to guard against potential adverse risk selection.},
archivePrefix = {arXiv},
arxivId = {1904.10890},
author = {Henckaerts, Roel and C{\^{o}}t{\'{e}}, Marie Pier and Antonio, Katrien and Verbelen, Roel},
doi = {10.1080/10920277.2020.1745656},
eprint = {1904.10890},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Henckaerts et al. - 2020 - Boosting Insights in Insurance Tariff Plans with Tree-Based Machine Learning Methods(2).pdf:pdf},
issn = {10920277},
journal = {North American Actuarial Journal},
keywords = {cross-validation,deviance,frequency,gradient boosting,interpretable machine learning,machine,model lift,severity modeling},
pages = {1--31},
title = {{Boosting Insights in Insurance Tariff Plans with Tree-Based Machine Learning Methods}},
year = {2020}
}
@article{Holvoet2020,
abstract = {Poster},
author = {Holvoet, Freek},
title = {{Use of Neural Network Models in MTPL Insurance Pricing}},
year = {2020}
}
@article{Holvoet2021,
author = {Holvoet, Freek},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Holvoet - 2021 - Use of Neural Networks in MTPL Insurance Pricing.pdf:pdf},
title = {{Use of Neural Networks in MTPL Insurance Pricing}},
year = {2021}
}
@article{hornik1989,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. {\textcopyright} 1989.},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
file = {:C\:/Users/Frynn/Dropbox/Freek research project/Thesis and Papers/hornik1984.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
month = {jan},
number = {5},
pages = {359--366},
title = {{Multilayer feedforward networks are universal approximators}},
url = {https://linkinghub.elsevier.com/retrieve/pii/0893608089900208},
volume = {2},
year = {1989}
}
@article{Hutchinson1994,
abstract = {We propose a nonparametric method for estimating the pricing formula of a derivative asset using learning networks. Although not a substitute for the more traditional arbitrage-based pricing formulas, network-pricing formulas may be more accurate and computationally more efficient alternatives when the underlying asset's price dynamics are unknown, or when the pricing equation associated with the no-arbitrage condition cannot be solved analytically. To assess the potential value of network pricing formulas, we simulate Black-Scholes option prices and show that learning networks can recover the Black-Scholes formula from a two-year training set of daily options prices, and that the resulting network formula can be used successfully to both price and delta-hedge options out-of-sample. For comparison, we estimate models using four popular methods: ordinary least squares, radial basis function networks, multilayer perceptron networks, and projection pursuit. To illustrate the practical relevance of our network pricing approach, we apply it to the pricing and delta-hedging of S&P 500 futures options from 1987 to 1991.},
author = {Hutchinson, James M. and Lo, Andrew W. and Poggio, Tomaso},
doi = {10.2307/2329209},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hutchinson, Lo, Poggio - 1994 - A Nonparametric Approach to Pricing and Hedging Derivative Securities Via Learning Networks.pdf:pdf},
issn = {00221082},
journal = {The Journal of Finance},
number = {3},
pages = {851},
title = {{A Nonparametric Approach to Pricing and Hedging Derivative Securities Via Learning Networks}},
volume = {49},
year = {1994}
}
@article{Hutchinson1994a,
abstract = {We propose a nonparametric method for estimating the pricing formula of a derivative asset using learning networks. Although not a substitute for the more traditional arbitrage-based pricing formulas, network-pricing formulas may be more accurate and computationally more efficient alternatives when the underlying asset's price dynamics are unknown, or when the pricing equation associated with the no-arbitrage condition cannot be solved analytically. To assess the potential value of network pricing formulas, we simulate Black-Scholes option prices and show that learning networks can recover the Black-Scholes formula from a two-year training set of daily options prices, and that the resulting network formula can be used successfully to both price and delta-hedge options out-of-sample. For comparison, we estimate models using four popular methods: ordinary least squares, radial basis function networks, multilayer perceptron networks, and projection pursuit. To illustrate the practical relevance of our network pricing approach, we apply it to the pricing and delta-hedging of S&P 500 futures options from 1987 to 1991.},
author = {Hutchinson, James M. and Lo, Andrew W. and Poggio, Tomaso},
doi = {10.2307/2329209},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hutchinson, Lo, Poggio - 1994 - A Nonparametric Approach to Pricing and Hedging Derivative Securities Via Learning Networks.pdf:pdf},
issn = {00221082},
journal = {The Journal of Finance},
number = {3},
pages = {851},
title = {{A Nonparametric Approach to Pricing and Hedging Derivative Securities Via Learning Networks}},
volume = {49},
year = {1994}
}
@article{keskar2016,
abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say $32$-$512$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
archivePrefix = {arXiv},
arxivId = {1609.04836},
author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
eprint = {1609.04836},
journal = {Proceedings of ICLR},
month = {sep},
pages = {1--16},
title = {{On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}},
url = {http://arxiv.org/abs/1609.04836},
year = {2017}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - 2015 - Adam A method for stochastic optimization.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--15},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}
@article{klein2014,
author = {Klein, Nadja and Denuit, Michel and Lang, Stefan and Kneib, Thomas},
doi = {10.1016/j.insmatheco.2014.02.001},
issn = {01676687},
journal = {Insurance: Mathematics and Economics},
month = {mar},
pages = {225--249},
title = {{Nonlife ratemaking and risk management with Bayesian generalized additive models for location, scale, and shape}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0167668714000183},
volume = {55},
year = {2014}
}
@book{klugman2012,
author = {Klugman, Stuart A. and Panjer, Harry H. and Willmot, Gordon E.},
doi = {10.1002/9780470391341},
isbn = {9780470391341},
month = {sep},
publisher = {Wiley},
series = {Wiley Series in Probability and Statistics},
title = {{Loss Models}},
url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470391341},
year = {2008}
}
@article{Lorentzen2020,
abstract = {This tutorial gives an overview of tools for explaining and interpreting black box machine learning models like boosted trees or deep neural networks. All our methods are illustrated on a publicly available real car insurance data set on claims frequencies.},
author = {Lorentzen, Christian and Mayer, Michael},
doi = {10.2139/ssrn.3595944},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lorentzen, Mayer - 2020 - Peeking into the Black Box An Actuarial Case Study for Interpretable Machine Learning.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {black,box models,claims,explainability,flashlight,interpretability,machine learning,model-agnostic technique,motor insurance,xai},
pages = {1--40},
title = {{Peeking into the Black Box: An Actuarial Case Study for Interpretable Machine Learning}},
url = {https://www.ssrn.com/abstract=3595944},
year = {2020}
}
@article{Marin-galiano2004,
abstract = {Data sets from car insurance companies often have a high-dimensional complex dependency structure. The use of classical statistical methods such as generalized linear models or Tweedie's compound Poisson model can yield problems in this case. Christmann (2004) proposed a general approach to model the pure premium by exploiting characteristic features of such data sets. In this paper we describe a program to use this approach based on a combination of multinomial logistic regression and $\epsilon$−support vector regres- sion from modern statistical machine learning. Keywords:},
author = {Marin-galiano, Marcos and Christmann, Andreas},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marin-galiano, Christmann - 2004 - Insurance an R-Program to Model Insurance Data.pdf:pdf},
journal = {Technical Report/Universit{\"{a}}t Dortmund},
keywords = {claim size,insurance tariff,learning,logistic regression,statistical machine,support vector regression},
pages = {1--26},
title = {{Insurance : an R-Program to Model Insurance Data}},
year = {2004}
}
@article{MCCULLOCH1990,
abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McCulloch, Pitts - 2017 - A logical calculus of the ideas immanent in nervous activity.pdf:pdf},
isbn = {0007-4985},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
month = {dec},
number = {4},
pages = {115--133},
pmid = {2185863},
publisher = {Taylor and Francis},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {http://link.springer.com/10.1016/S0092-8240(05)80006-0 http://link.springer.com/10.1007/BF02478259},
volume = {5},
year = {1943}
}
@book{molnar2019,
author = {Molnar, Christoph},
publisher = {christophm.github.io/interpretable-ml-book},
title = {{Interpretable Machine Learning}},
url = {https://christophm.github.io/interpretable-ml-book/},
year = {2019}
}
@article{Society2013,
abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log‐likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.},
author = {Nelder, J. A. and Wedderburn, R. W. M.},
doi = {10.2307/2344614},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nelder, Wedderburn - 1972 - Generalized Linear Models.pdf:pdf},
issn = {00359238},
journal = {Journal of the Royal Statistical Society. Series A (General)},
keywords = {beta binomial distribution,brand choice,buyer behaviour,consumer purchasing,dirichlet distribution,distribution,gamma distribution,incidence,multinomial distribution,multivariate beta,negative binomial distribution,poisson distribution,purchase,stochastic model},
number = {3},
pages = {370--384},
title = {{Generalized Linear Models}},
url = {https://www.jstor.org/stable/10.2307/2344614?origin=crossref},
volume = {135},
year = {1972}
}
@article{Olden2004,
abstract = {Artificial neural networks (ANNs) are receiving greater attention in the ecological sciences as a powerful statistical modeling technique; however, they have also been labeled a "black box" because they are believed to provide little explanatory insight into the contributions of the independent variables in the prediction process. A recent paper published in Ecological Modelling [Review and comparison of methods to study the contribution of variables in artificial neural network models, Ecol. Model. 160 (2003) 249-264] addressed this concern by providing a comprehensive comparison of eight different methodologies for estimating variable importance in neural networks that are commonly used in ecology. Unfortunately, comparisons of the different methodologies were based on an empirical dataset, which precludes the ability to establish generalizations regarding the true accuracy and precision of the different approaches because the true importance of the variables is unknown. Here, we provide a more appropriate comparison of the different methodologies by using Monte Carlo simulations with data exhibiting defined (and consequently known) numeric relationships. Our results show that a Connection Weight Approach that uses raw input-hidden and hidden-output connection weights in the neural network provides the best methodology for accurately quantifying variable importance and should be favored over the other approaches commonly used in the ecological literature. Average similarity between true and estimated ranked variable importance using this approach was 0.92, whereas, similarity coefficients ranged between 0.28 and 0.74 for the other approaches. Furthermore, the Connection Weight Approach was the only method that consistently identified the correct ranked importance of all predictor variables, whereas, the other methods either only identified the first few important variables in the network or no variables at all. The most notably result was that Garson's Algorithm was the poorest performing approach, yet is the most commonly used in the ecological literature. In conclusion, this study provides a robust comparison of different methodologies for assessing variable importance in neural networks that can be generalized to other data and from which valid recommendations can be made for future studies. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Olden, Julian D. and Joy, Michael K. and Death, Russell G.},
doi = {10.1016/j.ecolmodel.2004.03.013},
issn = {03043800},
journal = {Ecological Modelling},
keywords = {Connection weights,Explanatory power,Garson's algorithm,Sensitivity analysis,Statistical models},
month = {nov},
number = {3-4},
pages = {389--397},
publisher = {Elsevier},
title = {{An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data}},
volume = {178},
year = {2004}
}
@article{Paefgen2013,
abstract = {Vehicle sensor data enable novel, usage-based insurance premium models known as 'Pay-As-You-Drive' (PAYD) insurance, but pose substantial challenges for actuarial decision-making because of their inherent complexity and volume. Based on a large real-world sample of location data from 1572 vehicles, the present study proposes a classification analysis approach that addresses (i) the selection of predictor variables, (ii) the presence of class skew and time-variant prior distributions, and (iii) the suitability of classifier scores as an aggregated actuarial rate factor. Using raw location data, we derive a set of 15 predictor variables that we use to train and compare logistic regression, neural network, and decision tree classifiers. We find that while neural networks exhibit superior classification performance, logistic regression is better suited from an actuarial viewpoint in several ways. In sum, our results clearly demonstrate the potential of high-resolution exposure data for reducing the complexity of PAYD insurance pricing in practice. {\textcopyright} 2013 Elsevier B.V.},
author = {Paefgen, Johannes and Staake, Thorsten and Thiesse, Fr{\'{e}}d{\'{e}}ric},
doi = {10.1016/j.dss.2013.06.001},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paefgen, Staake, Thiesse - 2013 - Evaluation and aggregation of pay-as-you-drive insurance rate factors A classification analysis approa.pdf:pdf},
issn = {01679236},
journal = {Decision Support Systems},
keywords = {Actuarial decision-making,Classification analysis,Decision trees,Location data,Logistic regression,Neural networks,Pay-as-you-drive insurance},
number = {1},
pages = {192--201},
publisher = {Elsevier B.V.},
title = {{Evaluation and aggregation of pay-as-you-drive insurance rate factors: A classification analysis approach}},
url = {http://dx.doi.org/10.1016/j.dss.2013.06.001},
volume = {56},
year = {2013}
}
@book{parodi2014,
author = {Parodi, Pietro},
doi = {10.1201/b17525},
isbn = {9781466581487},
month = {oct},
publisher = {Chapman and Hall},
title = {{Pricing in General Insurance}},
url = {https://www.taylorfrancis.com/books/9781466581487},
year = {2014}
}
@article{richman2018,
author = {Richman, Ronald},
doi = {10.2139/ssrn.3218082},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
pages = {1--53},
title = {{AI in Actuarial Science}},
url = {https://www.ssrn.com/abstract=3218082},
year = {2018}
}
@article{Ruder2016,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747},
author = {Ruder, Sebastian},
eprint = {1609.04747},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruder - 2016 - An overview of gradient descent optimization algorithms.pdf:pdf},
journal = {Internal Report},
month = {sep},
pages = {1--14},
title = {{An overview of gradient descent optimization algorithms}},
url = {arxiv:1609.04747v2 http://arxiv.org/abs/1609.04747},
year = {2016}
}
@article{Schelldorfer2019,
abstract = {Neural network modeling often suffers the deficiency of not using a systematic way of improving classical statistical regression models. In this tutorial we exemplify the proposal of [17]. We embed a classical generalized linear model into a neural network architecture, and we let this nested network approach explore model structure not captured by the classical generalized linear model. In addition, if the generalized linear model is already close to optimal , then the maximum likelihood estimator of the generalized linear model can be used as initialization of the fitting algorithm of the neural network. This saves computational time because we start the fitting algorithm in a reasonable parameter. As a by-product of our derivations, we present embedding layers and representation learning which often provides a more efficient treatment of categorical features within neural networks than dummy and one-hot encoding.},
author = {Schelldorfer, J{\"{u}}rg and W{\"{u}}thrich, Mario V.},
doi = {10.2139/ssrn.3320525},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schelldorfer, Wuthrich - 2019 - Nesting Classical Actuarial Models into Neural Networks.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {architecture,car insurance,claims frequency,deep learning,ding,dummy coding,embed-,embedding layers,generalized linear models,machine learning,nesting,neural networks,one-hot encoding,poisson regression model,representation learning},
pages = {1--27},
title = {{Nesting Classical Actuarial Models into Neural Networks}},
year = {2019}
}
@article{smyth2002,
abstract = {We reconsider the problem of producing fair and accurate tariffs based on aggregated insurance data giving numbers of claims and total costs for the claims. J{\o}rgensen and de Souza ( Scand Actuarial J. , 1994) assumed Poisson arrival of claims and gamma distributed costs for individual claims. J{\o}rgensen and de Souza (1994) directly modelled the risk or expected cost of claims per insured unit, $\mu$ say. They observed that the dependence of the likelihood function on $\mu$ is as for a linear exponential family, so that modelling similar to that of generalized linear models is possible. In this paper we observe that, when modelling the cost of insurance claims, it is generally necessary to model the dispersion of the costs as well as their mean. In order to model the dispersion we use the framework of double generalized linear models. Modelling the dispersion increases the precision of the estimated tariffs. The use of double generalized linear models also allows us to handle the case where only the total cost of claims and not the number of claims has been recorded.},
author = {Smyth, Gordon K and J{\o}rgensen, Bent},
doi = {10.2143/ast.32.1.1020},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Smyth, J{\o}rgensen - 2002 - Fitting Tweedie's Compound Poisson Model to Insurance Claims Data Dispersion Modelling.pdf:pdf},
issn = {0515-0361},
journal = {ASTIN Bulletin},
keywords = {Car insurance,Claims data,Compound Poisson model,Dispersion modelling,Double generalized linear models,Exposure,Generalized linear models,Power variance function,REML,Risk theory,Tarification},
number = {1},
pages = {143--157},
title = {{Fitting Tweedie's Compound Poisson Model to Insurance Claims Data: Dispersion Modelling}},
url = {https://doi.org/10.2143/AST.32.1.1020},
volume = {32},
year = {2002}
}
@article{dropout2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.5555/2670313},
file = {:C\:/Users/Frynn/Dropbox/Freek research project/Thesis and Papers/JMLRdropout.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@book{Steinwart2008,
abstract = {This book explains the principles that make support vector machines (SVMs) a successful modelling and prediction tool for a variety of applications. The authors present the basic ideas of SVMs together with the latest developments and current research questions in a unified style. They identify three reasons for the success of SVMs: their ability to learn well with only a very small number of free parameters, their robustness against several types of model violations and outliers, and their computational efficiency compared to several other methods. Since their appearance in the early nineties, support vector machines and related kernel-based methods have been successfully applied in diverse fields of application such as bioinformatics, fraud detection, construction of insurance tariffs, direct marketing, and data and text mining. As a consequence, SVMs now play an important role in statistical machine learning and are used not only by statisticians, mathematicians, and computer scientists, but also by engineers and data analysts. The book provides a unique in-depth treatment of both fundamental and recent material on SVMs that so far has been scattered in the literature. The book can thus serve as both a basis for graduate courses and an introduction for statisticians, mathematicians, and computer scientists. It further provides a valuable reference for researchers working in the field. The book covers all important topics concerning support vector machines such as: loss functions and their role in the learning process; reproducing kernel Hilbert spaces and their properties; a thorough statistical analysis that uses both traditional uniform bounds and more advanced localized techniques based on Rademacher averages and Talagrand's inequality; a detailed treatment of classification and regression; a detailed robustness analysis; and a description of some of the most recent implementation techniques. To make the book self-contained, an extensive appendix is added which provides the reader with the necessary background from statistics, probability theory, functional analysis, convex analysis, and topology. Ingo Steinwart is a researcher in the machine learning group at the Los Alamos National Laboratory. He works on support vector machines and related methods. Andreas Christmann is Professor of Stochastics in the Department of Mathematics at the University of Bayreuth. He works in particular on support vector machines and robust statistics.},
address = {New York, NY},
author = {Steinwart, Ingo and Christmann, Andreas},
doi = {10.1007/978-0-387-77242-4},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Steinwart, Christmann - 2008 - Support Vector Machines.pdf:pdf},
isbn = {978-0-387-77241-7},
issn = {1613-9011},
publisher = {Springer New York},
series = {Information Science and Statistics},
title = {{Support Vector Machines}},
url = {http://link.springer.com/10.1007/978-0-387-77242-4},
year = {2008}
}
@book{venables2002,
address = {New York, NY},
author = {Venables, W. N. and Ripley, B. D.},
doi = {10.1007/978-0-387-21706-2},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Venables, Ripley - 2002 - Modern Applied Statistics with S.pdf:pdf},
isbn = {978-1-4419-3008-8},
pages = {413--430},
publisher = {Springer},
series = {Statistics and Computing},
title = {{Modern Applied Statistics with S}},
url = {http://link.springer.com/10.1007/978-1-4757-2719-7_14 http://link.springer.com/10.1007/b97626 http://link.springer.com/10.1007/978-0-387-21706-2},
year = {2002}
}
@article{Wuthrich2019,
abstract = {We present how to enhance classical generalized linear models by neural network features. On the way there, we highlight the traps and pitfalls that need to be avoided to get good statistical models. This includes the non-uniqueness of sufficiently good regression models, the balance property, and representation learning, which brings us back to the concept of the good old generalized linear models.},
author = {W{\"{u}}thrich, Mario V.},
doi = {10.2139/ssrn.3491790},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wuthrich - 2019 - From Generalized Linear Models to Neural Networks, and Back(2).pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {balance property,canonical link,claims frequency modeling,deviance loss,exponen-,generalized linear model,glm,lasso,neural network,regression modeling,regularization,representation learning,tial dispersion family},
pages = {1--56},
title = {{From Generalized Linear Models to Neural Networks, and Back}},
year = {2019}
}
@book{Wuthrich2014,
abstract = {The present notes aim at providing a basis in non-life insurance mathematics which forms a core subject of actuarial sciences. It discusses collective risk modeling, individual claim size modeling, approximations for compound distributions, ruin theory, premium calculation principles, tariffication with generalized linear models, credibility theory, claims reserving and solvency.},
author = {W{\"{u}}thrich, Mario V.},
booktitle = {SSRN Electronic Journal},
doi = {10.2139/ssrn.2319328},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/W{\"{u}}thrich - 2013 - Non-Life Insurance Mathematics & Statistics.pdf:pdf},
issn = {1556-5068},
title = {{Non-Life Insurance: Mathematics & Statistics}},
url = {http://www.ssrn.com/abstract=2319328},
year = {2013}
}
@article{Wuthrich2020,
abstract = {Generalized linear models have the important property of providing unbiased estimates on a portfolio level. This implies that generalized linear models manage to provide accurate prices on a portfolio level. On the other hand, neural networks may provide very accurate prices on an individual policy level, but state-of-the-art use of neural networks does not pay any attention to unbiasedness on the portfolio level. This is an implicit consequence of applying early stopping rules in gradient descent methods for model fitting. In the present paper we discuss this deficiency and we provide two different techniques to overcome this drawback of neural network model fitting.},
author = {W{\"{u}}thrich, Mario V.},
doi = {10.1007/s13385-019-00215-z},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/W{\"{u}}thrich - 2020 - Bias regularization in neural network models for general insurance pricing(2).pdf:pdf},
issn = {21909741},
journal = {European Actuarial Journal},
keywords = {Balance property,Exponential dispersion family,Generalized linear model,Gradient descent method,Neural network,Regression tree,Unbiasedness},
number = {1},
pages = {179--202},
title = {{Bias regularization in neural network models for general insurance pricing}},
volume = {10},
year = {2020}
}
@article{wuthrichbuser2019,
abstract = {These notes aim at giving a broad skill set to the actuarial profession in non-life insurance pricing and data science. We start from the classical world of generalized linear models, generalized additive models and credibility theory. These methods form the basis of the deeper statistical understanding. We then present several machine learning techniques such as regression trees, bagging, random forest, boosting machines, neural networks and support vector machines. Finally, we provide methodologies for analysing telematics car driving data from unsupervised learning.},
author = {W{\"{u}}thrich, Mario V. and Buser, Christoph},
doi = {10.2139/ssrn.2870308},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wuthrich, Buser - 2018 - Data Analytics for Non-Life Insurance Pricing.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
title = {{Data Analytics for Non-Life Insurance Pricing}},
url = {https://www.ssrn.com/abstract=2870308},
year = {2018}
}
@techreport{yang2016,
abstract = {The Tweedie GLM is a widely used method for predicting insurance premiums. However, the structure of the logarithmic mean is restricted to a linear form in the Tweedie GLM, which can be too rigid for many applications. As a better alternative, we propose a gradient tree-boosting algorithm and apply it to Tweedie compound Poisson models for pure premiums. We use a profile likelihood approach to estimate the index and dispersion parameters. Our method is capable of fitting a flexible nonlinear Tweedie model and capturing complex interactions among predictors. A simulation study confirms the excellent prediction performance of our method. As an application, we apply our method to an auto insurance claim data and show that the new method is superior to the existing methods in the sense that it generates more accurate premium predictions, thus helping solve the adverse selection issue. We have implemented our method in a user-friendly R package that also includes a nice visualization tool for interpreting the fitted model.},
archivePrefix = {arXiv},
arxivId = {1508.06378v3},
author = {Yang, Yi and Qian, Wei and Zou, Hui},
eprint = {1508.06378v3},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Qian, Zou - 2016 - Insurance Premium Prediction via Gradient Tree-Boosted Tweedie Compound Poisson Models.pdf:pdf},
title = {{Insurance Premium Prediction via Gradient Tree-Boosted Tweedie Compound Poisson Models}},
year = {2016}
}
@article{zochbauer2016,
abstract = {Today, generalized linear models (GLM) are the standard methods in pricing of non-life insurance products. However they require detailed knowledge about the structure of the data, which needs to be provided a priori by the pricing actuary. In this thesis, we present more flexible tree-based methods, that do not su↵er from this drawback. In the first part, we discuss their theoretical properties. In the second part, we apply the methods to the problem of predicting the claims frequency of a policyholder. The empirical analysis is based on the Motor Insurance Collision Data Set of AXA Winterthur. We conclude that the regression tree methodology provides a useful tool for building homogeneous subportfolios with similar risk characteristics. The more advanced tree ensemble methods such as bagging and random forest provide competitive alternatives to GLM models in terms of predictive capacity.},
author = {Zochbauer, Patrick},
file = {:C\:/Users/Frynn/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zochbauer - 2016 - Data Science in Non-Life Insurance Pricing Predicting Claims Frequencies using Tree-Based Models.pdf:pdf},
number = {July},
title = {{Data Science in Non-Life Insurance Pricing: Predicting Claims Frequencies using Tree-Based Models}},
url = {https://www.ethz.ch/content/dam/ethz/special-interest/math/imsf-dam/documents/walter-saxer-preis/ma-zoechbauer.pdf},
year = {2016}
}
